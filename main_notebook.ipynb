{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":64148,"databundleVersionId":7669720,"sourceType":"competition"},{"sourceId":168178971,"sourceType":"kernelVersion"},{"sourceId":10417,"sourceType":"modelInstanceVersion","modelInstanceId":8385,"modelId":3301},{"sourceId":11264,"sourceType":"modelInstanceVersion","modelInstanceId":8318,"modelId":3301}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Data Science AI Assistant with Gemma 2b-it","metadata":{}},{"cell_type":"markdown","source":"# 1. What is a RAG and how it can help to explain or teach basic data science concepts","metadata":{}},{"cell_type":"markdown","source":"A **Retrieval-Augmented Generation (RAG)** is a solution that improves text generation of a large language model by integrating its answers using some kind of external knowledge retrieval.\n\nHence, it combines a **retriever** to fetch relevant information and a **generator** to produce accurate responses based on this retrieved knowledge. Basically, it is just like first doing a search engine query (the retriever), getting the best answers, and then asking a large language model such as **Gemma** or **Gemini** to process the information (generator) to answer an initial question.","metadata":{}},{"cell_type":"markdown","source":"![High-level RAG architecture](https://raw.githubusercontent.com/lmassaron/useful_stuff/main/High-Level%20RAG%20Architecture_rev2.jpg)","metadata":{}},{"cell_type":"markdown","source":"Such an approach ensures AI models have access to up-to-date and relevant facts, improving the quality and reliability of their generated text, especially in tasks like question-answering where factual accuracy is crucial and LLMs are infamous for sometimes coming up with made-up information (hallucinations).\n\nIn this case, **Google Gemma** seems already quite apt at answering basic questions about data science, but the idea is to further improve its competencies by providing it reliable information about AI, statistics, machine learning, and data science in general.","metadata":{}},{"cell_type":"markdown","source":"# 2. Setting up the necessary stuff","metadata":{}},{"cell_type":"markdown","source":"In the first cell of this notebook, some key packages for the project are installed or updated to the latest version.\n","metadata":{}},{"cell_type":"code","source":"!pip install -q -U torch --index-url https://download.pytorch.org/whl/cu117\n!pip install -q -U transformers==\"4.38.2\"\n!pip install -q accelerate\n!pip install -q -i https://pypi.org/simple/ bitsandbytes\n!pip install -q -U sentence_transformers\n!pip install -q -U scann\n!pip install -q -U wikipedia-api","metadata":{"execution":{"iopub.status.busy":"2025-01-29T08:23:44.808248Z","iopub.execute_input":"2025-01-29T08:23:44.808620Z","iopub.status.idle":"2025-01-29T08:25:31.636701Z","shell.execute_reply.started":"2025-01-29T08:23:44.808579Z","shell.execute_reply":"2025-01-29T08:25:31.635838Z"},"trusted":true},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.7/130.7 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m93.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m88.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nsentence-transformers 3.3.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.38.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.7/69.7 MB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m275.7/275.7 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m107.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m79.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m615.3/615.3 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m102.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-decision-forests 1.10.0 requires tensorflow==2.17.0, but you have tensorflow 2.18.0 which is incompatible.\ntensorflow-text 2.17.0 requires tensorflow<2.18,>=2.17.0, but you have tensorflow 2.18.0 which is incompatible.\ntf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.18.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for wikipedia-api (setup.py) ... \u001b[?25l\u001b[?25hdone\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"","metadata":{"execution":{"iopub.status.busy":"2025-01-29T08:25:31.637885Z","iopub.execute_input":"2025-01-29T08:25:31.638134Z","iopub.status.idle":"2025-01-29T08:25:31.642094Z","shell.execute_reply.started":"2025-01-29T08:25:31.638111Z","shell.execute_reply":"2025-01-29T08:25:31.641205Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2025-01-29T08:25:35.836191Z","iopub.execute_input":"2025-01-29T08:25:35.836534Z","iopub.status.idle":"2025-01-29T08:25:35.840074Z","shell.execute_reply.started":"2025-01-29T08:25:35.836504Z","shell.execute_reply":"2025-01-29T08:25:35.839176Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"In the next cell, the notebook loads Python libraries and modules for natural language processing tasks. It also includes libraries like **re** for regular expressions, **numpy** and **pandas** for data manipulation, **tqdm** for progress bars, **scann** for approximate nearest neighbor search, and **wikipediaapi** for accessing Wikipedia content (yes, we are going to use Wikipedia as a knowledge base).\n","metadata":{}},{"cell_type":"code","source":"import re\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport scann\nimport wikipediaapi\n\nimport torch\n\nimport transformers\nfrom transformers import (AutoModelForCausalLM, \n                          AutoTokenizer, \n                          BitsAndBytesConfig,\n                         )\nfrom sentence_transformers import SentenceTransformer\nimport bitsandbytes as bnb","metadata":{"execution":{"iopub.status.busy":"2025-01-29T08:25:38.616251Z","iopub.execute_input":"2025-01-29T08:25:38.616566Z","iopub.status.idle":"2025-01-29T08:25:57.222686Z","shell.execute_reply.started":"2025-01-29T08:25:38.616539Z","shell.execute_reply":"2025-01-29T08:25:57.221962Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# 3. Proceeding by building blocks","metadata":{}},{"cell_type":"markdown","source":"Before proceeding with the notebook, it is necessary to spend a word about how I will proceed in building the solution in a way that can be clear, easily explainable, and both reusable as well as hackable.","metadata":{}},{"cell_type":"markdown","source":"The AI assistant will simply be a class containing all you need for it to work and with methods for changing some settings (such as the temperature, which corresponds to its creativity, or the impersonated role, which influences how it responds) and for asking questions.\n\nAll the internal functions, however, are external, hence they are easier to present as stand-alone code snippets, easily reusable for different purposes or projects, and easily upgradable or hackable. Because as you change an external function, you immediately change the behavior of the class, without having to reinstantiate it again (it actually takes some time to re-index all the knowledge base, which may prevent some fast experimentation).\n","metadata":{}},{"cell_type":"code","source":"def define_device():\n    \"\"\"Define the device to be used by PyTorch\"\"\"\n\n    # Get the PyTorch version\n    torch_version = torch.__version__\n\n    # Print the PyTorch version\n    print(f\"PyTorch version: {torch_version}\", end=\" -- \")\n\n    # Check if MPS (Multi-Process Service) device is available on MacOS\n    if torch.backends.mps.is_available():\n        # If MPS is available, print a message indicating its usage\n        print(\"using MPS device on MacOS\")\n        # Define the device as MPS\n        defined_device = torch.device(\"mps\")\n    else:\n        # If MPS is not available, determine the device based on GPU availability\n        defined_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        # Print a message indicating the selected device\n        print(f\"using {defined_device}\")\n\n    # Return the defined device\n    return defined_device\n","metadata":{"execution":{"iopub.status.busy":"2025-01-29T08:25:59.519995Z","iopub.execute_input":"2025-01-29T08:25:59.520679Z","iopub.status.idle":"2025-01-29T08:25:59.525540Z","shell.execute_reply.started":"2025-01-29T08:25:59.520644Z","shell.execute_reply":"2025-01-29T08:25:59.524604Z"},"trusted":true},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"The next cells, instead, present two functions designed to operate using the **SentenceTransformers** package ([the package home page](https://www.sbert.net/index.html)), that can operate with lists of text and map them into embeddings.\n","metadata":{}},{"cell_type":"markdown","source":"Embeddings, such as those processed by packages like **SentenceTransformers**, are representations of text or sentences in a numerical form that capture their semantic meaning. These embeddings are created by transforming words or sentences into high-dimensional vectors, where similar vectors represent similar meanings.","metadata":{}},{"cell_type":"markdown","source":"In the context of **SentenceTransformers**, these embeddings are generated using models like BERT or XLNet that have been fine-tuned to produce meaningful sentence representations. These embeddings can be used for various tasks like clustering, semantic textual similarity, and information retrieval (in our project we actually need a retrieval function) by comparing the vectors using metrics like cosine similarity.","metadata":{}},{"cell_type":"code","source":"def get_embedding(text, embedding_model):\n    \"\"\"Get embeddings for a given text using the provided embedding model\"\"\"\n    \n    # Encode the text to obtain embeddings using the provided embedding model\n    embedding = embedding_model.encode(text, show_progress_bar=False)\n    \n    # Convert the embeddings to a list of floats and return\n    return embedding.tolist()\n\ndef map2embeddings(data, embedding_model):\n    \"\"\"Map a list of texts to their embeddings using the provided embedding model\"\"\"\n    \n    # Initialize an empty list to store embeddings\n    embeddings = []\n\n    # Iterate over each text in the input data list\n    no_texts = len(data)\n    print(f\"Mapping {no_texts} pieces of information\")\n    for i in tqdm(range(no_texts)):\n        # Get embeddings for the current text using the provided embedding model\n        embeddings.append(get_embedding(data[i], embedding_model))\n    \n    # Return the list of embeddings\n    return embeddings","metadata":{"execution":{"iopub.status.busy":"2025-01-29T08:26:02.986053Z","iopub.execute_input":"2025-01-29T08:26:02.986344Z","iopub.status.idle":"2025-01-29T08:26:02.991262Z","shell.execute_reply.started":"2025-01-29T08:26:02.986321Z","shell.execute_reply":"2025-01-29T08:26:02.990360Z"},"trusted":true},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def clean_text(txt, EOS_TOKEN):\n    \"\"\"Clean text by removing specific tokens and redundant spaces\"\"\"\n    txt = (txt\n           .replace(EOS_TOKEN, \"\") # Replace the end-of-sentence token with an empty string\n           .replace(\"**\", \"\")      # Replace double asterisks with an empty string\n           .replace(\"<pad>\", \"\")   # Replace \"<pad>\" with an empty string\n           .replace(\"  \", \" \")     # Replace double spaces with single spaces\n          ).strip()                # Strip leading and trailing spaces from the text\n    return txt","metadata":{"execution":{"iopub.status.busy":"2025-01-29T08:26:05.735285Z","iopub.execute_input":"2025-01-29T08:26:05.735622Z","iopub.status.idle":"2025-01-29T08:26:05.740254Z","shell.execute_reply.started":"2025-01-29T08:26:05.735594Z","shell.execute_reply":"2025-01-29T08:26:05.739232Z"},"trusted":true},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"The following function, instead, simply adds an indefinite article to a role name, something useful to make a prompt nicer and easier to read.\n","metadata":{}},{"cell_type":"code","source":"def add_indefinite_article(role_name):\n    \"\"\"Check if a role name has a determinative adjective before it, and if not, add the correct one\"\"\"\n    \n    # Check if the first word is a determinative adjective\n    determinative_adjectives = [\"a\", \"an\", \"the\"]\n    words = role_name.split()\n    if words[0].lower() not in determinative_adjectives:\n        # Use \"a\" or \"an\" based on the first letter of the role name\n        determinative_adjective = \"an\" if words[0][0].lower() in \"aeiou\" else \"a\"\n        role_name = f\"{determinative_adjective} {role_name}\"\n\n    return role_name","metadata":{"execution":{"iopub.status.busy":"2025-01-29T08:26:08.428177Z","iopub.execute_input":"2025-01-29T08:26:08.428520Z","iopub.status.idle":"2025-01-29T08:26:08.433140Z","shell.execute_reply.started":"2025-01-29T08:26:08.428492Z","shell.execute_reply":"2025-01-29T08:26:08.432178Z"},"trusted":true},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"After the previous functions, mostly devoted to processing text for better readability, the next class helps first to load and initialize Gemma by quantizing it to 4-bit, reducing its memory footprint and allowing for faster responses, and then to generate text from it. <u>**Gemma** is the core of our generative functions</u>, making it a crucial element for processing information and returning it to the user in the most usable and useful form.\n","metadata":{}},{"cell_type":"markdown","source":"The `GemmaHF` class serves as a wrapper for the Transformers implementation of Gemma. Upon initialization, it sets up the model and tokenizer using the specified model name and a maximum sequence length for the tokenizer. \n\nIn short, the method `initialize_model` is designed to set up a 4-bit quantized causal language model (LLM) and tokenizer and configure them. It begins by defining the data type for computation as `float16`. Then, it creates a configuration for quantization using the `BitsAndBytesConfig` class with settings for 4-bit quantization. The function loads a pre-trained model (Gemma 2b-it in the project, but you can try also the 7b version) with the specified quantization configuration. It also loads a tokenizer with the selected device mapping and maximum sequence length settings. Finally, the method returns the initialized model and tokenizer, ready for use by our AI assistant.\n\nFinally, its `generate_text` method takes a prompt as input and generates a text using the instantiated tokenizer and model, allowing for customization of parameters such as maximum new tokens and temperature for sampling. Under the hood, it encodes the prompt, generates text based on it, decodes the output into text, and returns a list of generated text results.\n","metadata":{}},{"cell_type":"code","source":"class GemmaHF():\n    \"\"\"Wrapper for the Transformers implementation of Gemma\"\"\"\n    \n    def __init__(self, model_name, max_seq_length=2048):\n        self.model_name = model_name\n        self.max_seq_length = max_seq_length\n        \n        # Initialize the model and tokenizer\n        print(\"\\nInitializing model:\")\n        self.device = define_device()\n        self.model, self.tokenizer = self.initialize_model(self.model_name, self.device, self.max_seq_length)\n        \n    def initialize_model(self, model_name, device, max_seq_length):\n        \"\"\"Initialize a 4-bit quantized causal language model (LLM) and tokenizer with specified settings\"\"\"\n\n        # Define the data type for computation\n        compute_dtype = getattr(torch, \"float16\")\n\n        # Define the configuration for quantization\n        bnb_config = BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_use_double_quant=True,\n            bnb_4bit_quant_type=\"nf4\",\n            bnb_4bit_compute_dtype=compute_dtype,\n        )\n\n        # Load the pre-trained model with quantization configuration\n        model = AutoModelForCausalLM.from_pretrained(\n            model_name,\n            device_map=device,\n            quantization_config=bnb_config,\n        )\n\n        # Load the tokenizer with specified device and max_seq_length\n        tokenizer = AutoTokenizer.from_pretrained(\n            model_name,\n            device_map=device,\n            max_seq_length=max_seq_length\n        )\n        \n        # Return the initialized model and tokenizer\n        return model, tokenizer\n    \n    def generate_text(self, prompt, max_new_tokens=2048, temperature=0.0):\n        \"\"\"Generate text using the instantiated tokenizer and model with specified settings\"\"\"\n    \n        # Encode the prompt and convert to PyTorch tensor\n        input_ids = self.tokenizer(prompt, return_tensors=\"pt\", padding=True).to(self.device)\n\n        # Determine if sampling should be performed based on temperature\n        do_sample = True if temperature > 0 else False\n\n        # Generate text based on the input prompt\n        outputs = self.model.generate(**input_ids, \n                                      max_new_tokens=max_new_tokens, \n                                      do_sample=do_sample, \n                                      temperature=temperature\n                                     )\n\n        # Decode the generated output into text\n        results = [self.tokenizer.decode(output) for output in outputs]\n\n        # Return the list of generated text results\n        return results","metadata":{"execution":{"iopub.status.busy":"2025-01-29T08:26:11.305532Z","iopub.execute_input":"2025-01-29T08:26:11.305848Z","iopub.status.idle":"2025-01-29T08:26:11.312680Z","shell.execute_reply.started":"2025-01-29T08:26:11.305823Z","shell.execute_reply":"2025-01-29T08:26:11.311860Z"},"trusted":true},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"And here we arrive at the core of the generative function (before we just initialized the generative engine, Gemma).","metadata":{}},{"cell_type":"markdown","source":"The `generate_summary_and_answer` function, generates an answer for a given question using context from a dataset. It embeds the input question (using the `get_embedding` function we previously saw), finds similar contexts in the dataset, extracts relevant context based on similarity indices, generates prompts for summarizing the context and providing an answer, generates summaries and answers using the a generative method from a \"model\" class, which can be a wrapper class containing Gemma implementations based on HF Transformers, Keras, Gemma C++ or any other available. Afterwards, the function cleans the generated summary and answer, and returns the cleaned answer for further processing. This function works as a sequence of steps in order to generate informative responses starting from an input question and some knowledge base data previously provided.\n","metadata":{}},{"cell_type":"markdown","source":"The two-step execution processing the information retrieved from the knowledge base is necessary because extraction based on embedded vectors sometimes returns irrelevant information. It is a problem based on the fact that embeddings are a mapping that has many facets (they are high-dimensional themselves) and that distance measures, and methods for finding what documents or text are most similar to your question, are often approximate for performance reasons resulting sometimes in unexpected retrieved results. First summarizing relevant information, a task that Gemma can execute with prowess, helps in having a shorter, more compact, and surely more relevant context to provide to the further processing by Gemma, which consists of writing an answer to your question.\n","metadata":{}},{"cell_type":"markdown","source":"In this process, temperature, the level of creativity, and the role may result in different answers and also different answering styles. I decided to rely on the \"expert data scientist\" role, but you may decide for the \"ELI5 divulgator\" or the \"verbose scholarly narrator\"\n","metadata":{}},{"cell_type":"markdown","source":"Finally, notice the part of the generative prompt that says: \"If the context doesn't provide any relevant information, answer with <I couldn't find a good match in my knowledge base for your query, hence I answer based on my own knowledge>\". This is partly to prevent the assistant from losing its usefulness and to alert the user regarding the assistant providing peculiar answers when the question is off-topic, too difficult, or lacks sufficient information.\n","metadata":{}},{"cell_type":"code","source":"def generate_summary_and_answer(question, data, searcher, embedding_model, model,\n                                max_new_tokens=2048, temperature=0.4, role=\"expert\"):\n    \"\"\"Generate an answer for a given question using context from a dataset\"\"\"\n    \n    # Embed the input question using the provided embedding model\n    embeded_question = np.array(get_embedding(question, embedding_model)).reshape(1, -1)\n    \n    # Find similar contexts in the dataset based on the embedded question\n    neighbors, distances = searcher.search_batched(embeded_question)\n    \n    # Extract context from the dataset based on the indices of similar contexts\n    context = \" \".join([data[pos] for pos in np.ravel(neighbors)])\n    \n    # Get the end-of-sentence token from the tokenizer\n    try:\n        EOS_TOKEN = model.tokenizer.eos_token\n    except:\n        EOS_TOKEN = \"<eos>\"\n    \n    # Add a determinative adjective to the role\n    role = add_indefinite_article(role)\n    \n    # Generate a prompt for summarizing the context\n    prompt = f\"\"\"\n             Summarize this context: \"{context}\" in order to answer the question \"{question}\" as {role}\\\n             SUMMARY:\n             \"\"\".strip() + EOS_TOKEN\n    \n    # Generate a summary based on the prompt\n    results = model.generate_text(prompt, max_new_tokens, temperature)\n    \n    # Clean the generated summary\n    summary = clean_text(results[0].split(\"SUMMARY:\")[-1], EOS_TOKEN)\n\n    # Generate a prompt for providing an answer\n    prompt = f\"\"\"\n             Here is the context: {summary}\n             Using the relevant information from the context \n             and integrating it with your knowledge,\n             provide an answer as {role} to the question: {question}.\n             If the context doesn't provide\n             any relevant information answer with \n             [I couldn't find a good match in my\n             knowledge base for your question, \n             hence I answer based on my own knowledge] \\\n             ANSWER:\n             \"\"\".strip() + EOS_TOKEN\n\n    # Generate an answer based on the prompt\n    results = model.generate_text(prompt, max_new_tokens, temperature)\n    \n    # Clean the generated answer\n    answer = clean_text(results[0].split(\"ANSWER:\")[-1], EOS_TOKEN)\n\n    # Return the cleaned answer\n    return answer","metadata":{"execution":{"iopub.status.busy":"2025-01-29T08:26:15.360412Z","iopub.execute_input":"2025-01-29T08:26:15.360750Z","iopub.status.idle":"2025-01-29T08:26:15.367191Z","shell.execute_reply.started":"2025-01-29T08:26:15.360722Z","shell.execute_reply":"2025-01-29T08:26:15.366217Z"},"trusted":true},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"# 4. Wrapping up everything","metadata":{}},{"cell_type":"markdown","source":"At this point, the next cell wraps all the functions into an `AIAssistant` class.\n","metadata":{}},{"cell_type":"markdown","source":"The `AIAssistant` class impersonates an AI assistant that interacts with users by providing answers based on a given knowledge base (basically a list of texts containing the knowledge).\n\nUpon initialization, the class loads an embedding model, indexes the knowledge base for efficient search, initializes a language model and tokenizer, and builds a searcher for similarity search using the SCANN library. The class includes functions to query the knowledge base, adjust the assistant's temperature (creativity), and define its answering style.\n\n- The `query` function generates and prints an answer to a user query by utilizing the `generate_summary_and_answer` function.\n- The `set_temperature` function allows adjusting the assistant's creativity level, while the `set_role` function defines the answering style of the AI assistant.\n\nThis class wraps all together the functionality of an AI assistant that makes good use of embeddings, powerful language models such as Gemma, and similarity search to provide informative responses to user queries based on a predefined knowledge base.\n","metadata":{}},{"cell_type":"markdown","source":"A few notes about ScaNN. ScaNN is a library developed by Google Research that offers efficient and scalable nearest neighbor search capabilities. It provides advantages over other solutions by utilizing techniques like quantization and Anisotropic Hashing, which enhance search performance.\n\nAnisotropic Hashing is a method used in hashing techniques for multimodal retrieval that involves learning projection functions to produce dimensions with varying lengths or scales. This flexibility in scaling can be beneficial for capturing complex relationships and structures in high-dimensional data, offering improved retrieval performance in scenarios where isotropic methods may not be as effective. You can read everything about this method in the paper:\n\nGuo, Ruiqi, et al. \"Accelerating large-scale inference with anisotropic vector quantization.\" International Conference on Machine Learning. PMLR, 2020. ([Paper Link](https://arxiv.org/abs/1908.10396))\n\nor by browsing the code repository at [https://github.com/google-research/google-research/tree/master/scann](https://github.com/google-research/google-research/tree/master/scann)\n\nWhat is interesting to note is that in my solution I do not use the cosine distance but simply the dot product as suggested by this paper:\n\nSteck, Harald, Chaitanya Ekanadham, and Nathan Kallus. \"Is Cosine-Similarity of Embeddings Really About Similarity?.\" arXiv preprint arXiv:2403.05440 (2024). ([Paper Link](https://arxiv.org/html/2403.05440v1))\n\nAnd it works pretty well!\n","metadata":{}},{"cell_type":"code","source":"\nclass AIAssistant():\n    \"\"\"An AI assistant that interacts with users by providing answers based on a provided knowledge base\"\"\"\n    \n    def __init__(self, gemma_model, embeddings_name=\"thenlper/gte-large\", temperature=0.4, role=\"expert\"):\n        \"\"\"Initialize the AI assistant.\"\"\"\n        # Initialize attributes\n        self.embeddings_name = embeddings_name\n        self.knowledge_base = []\n        self.temperature = temperature\n        self.role = role\n        \n        # Initialize Gemma model (it can be transformer-based or any other)\n        self.gemma_model = gemma_model\n        \n        # Load the embedding model\n        self.embedding_model = SentenceTransformer(self.embeddings_name)\n        \n    def store_knowledge_base(self, knowledge_base):\n        \"\"\"Store the knowledge base\"\"\"\n        self.knowledge_base=knowledge_base\n        \n    def learn_knowledge_base(self, knowledge_base):\n        \"\"\"Store and index the knowledge based to be used by the assistant\"\"\"\n        # Storing the knowledge base\n        self.store_knowledge_base(knowledge_base)\n        \n        # Load and index the knowledge base\n        print(\"Indexing and mapping the knowledge base:\")\n        embeddings = map2embeddings(self.knowledge_base, self.embedding_model)\n        self.embeddings = np.array(embeddings).astype(np.float32)\n        \n        # Instantiate the searcher for similarity search\n        self.index_embeddings()\n        \n    def index_embeddings(self):\n        \"\"\"Index the embeddings using ScaNN \"\"\"\n        self.searcher = (scann.scann_ops_pybind.builder(db=self.embeddings, num_neighbors=10, distance_measure=\"dot_product\")\n                 .tree(num_leaves=min(self.embeddings.shape[0] // 2, 1000), \n                       num_leaves_to_search=100, \n                       training_sample_size=self.embeddings.shape[0])\n                 .score_ah(2, anisotropic_quantization_threshold=0.2)\n                 .reorder(100)\n                 .build()\n           )\n        \n    def query(self, query):\n        \"\"\"Query the knowledge base of the AI assistant.\"\"\"\n        # Generate and print an answer to the query\n        answer = generate_summary_and_answer(query, \n                                             self.knowledge_base, \n                                             self.searcher, \n                                             self.embedding_model, \n                                             self.gemma_model,\n                                             temperature=self.temperature,\n                                             role=self.role)\n        print(answer)\n        \n    def set_temperature(self, temperature):\n        \"\"\"Set the temperature (creativity) of the AI assistant.\"\"\"\n        self.temperature = temperature\n        \n    def set_role(self, role):\n        \"\"\"Define the answering style of the AI assistant.\"\"\"\n        self.role = role\n        \n    def save_embeddings(self, filename=\"embeddings.npy\"):\n        \"\"\"Save the embeddings to disk\"\"\"\n        np.save(filename, self.embeddings)\n        \n    def load_embeddings(self, filename=\"embeddings.npy\"):\n        \"\"\"Load the embeddings from disk and index them\"\"\"\n        self.embeddings = np.load(filename)\n        # Re-instantiate the searcher\n        self.index_embeddings()","metadata":{"execution":{"iopub.status.busy":"2025-01-29T08:26:19.834809Z","iopub.execute_input":"2025-01-29T08:26:19.835112Z","iopub.status.idle":"2025-01-29T08:26:19.843015Z","shell.execute_reply.started":"2025-01-29T08:26:19.835088Z","shell.execute_reply":"2025-01-29T08:26:19.842127Z"},"trusted":true},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"# 5. Providing the knowledge base from Wikipedia","metadata":{}},{"cell_type":"markdown","source":"In order to provide a **knowledge base** for the AI Assistant to work confidently with data science questions, I decided to retrieve some information from Wikipedia.","metadata":{}},{"cell_type":"markdown","source":"The following code, apart from the first two functions useful for cleaning the text from tags and formatting, extracts references, such as pages or other Wikipedia categories, using the `extract_wikipedia_pages` function. Then, the `get_wikipedia_pages` function takes care to crawl to all the pages and information related to some initial Wikipedia category or page.\n","metadata":{}},{"cell_type":"code","source":"# Pre-compile the regular expression pattern for better performance\nBRACES_PATTERN = re.compile(r'\\{.*?\\}|\\}')\n\ndef remove_braces_and_content(text):\n    \"\"\"Remove all occurrences of curly braces and their content from the given text\"\"\"\n    return BRACES_PATTERN.sub('', text)\n\ndef clean_string(input_string):\n    \"\"\"Clean the input string.\"\"\"\n    \n    # Remove extra spaces by splitting the string by spaces and joining back together\n    cleaned_string = ' '.join(input_string.split())\n    \n    # Remove consecutive carriage return characters until there are no more consecutive occurrences\n    cleaned_string = re.sub(r'\\r+', '\\r', cleaned_string)\n    \n    # Remove all occurrences of curly braces and their content from the cleaned string\n    cleaned_string = remove_braces_and_content(cleaned_string)\n    \n    # Return the cleaned string\n    return cleaned_string","metadata":{"execution":{"iopub.status.busy":"2025-01-29T08:26:24.965540Z","iopub.execute_input":"2025-01-29T08:26:24.965875Z","iopub.status.idle":"2025-01-29T08:26:24.970882Z","shell.execute_reply.started":"2025-01-29T08:26:24.965849Z","shell.execute_reply":"2025-01-29T08:26:24.969760Z"},"trusted":true},"outputs":[],"execution_count":14},{"cell_type":"code","source":"def extract_wikipedia_pages(wiki_wiki, category_name):\n    \"\"\"Extract all references from a category on Wikipedia\"\"\"\n    \n    # Get the Wikipedia page corresponding to the provided category name\n    category = wiki_wiki.page(\"Category:\" + category_name)\n    \n    # Initialize an empty list to store page titles\n    pages = []\n    \n    # Check if the category exists\n    if category.exists():\n        # Iterate through each article in the category and append its title to the list\n        for article in category.categorymembers.values():\n            pages.append(article.title)\n    \n    # Return the list of page titles\n    return pages","metadata":{"execution":{"iopub.status.busy":"2025-01-29T08:26:27.590715Z","iopub.execute_input":"2025-01-29T08:26:27.591023Z","iopub.status.idle":"2025-01-29T08:26:27.595330Z","shell.execute_reply.started":"2025-01-29T08:26:27.590999Z","shell.execute_reply":"2025-01-29T08:26:27.594475Z"},"trusted":true},"outputs":[],"execution_count":15},{"cell_type":"code","source":"def get_wikipedia_pages(categories):\n    \"\"\"Retrieve Wikipedia pages from a list of categories and extract their content\"\"\"\n    \n    # Create a Wikipedia object\n    wiki_wiki = wikipediaapi.Wikipedia('Gemma AI Assistant (gemma@example.com)', 'en')\n    \n    # Initialize lists to store explored categories and Wikipedia pages\n    explored_categories = []\n    wikipedia_pages = []\n\n    # Iterate through each category\n    print(\"- Processing Wikipedia categories:\")\n    for category_name in categories:\n        print(f\"\\tExploring {category_name} on Wikipedia\")\n        \n        # Get the Wikipedia page corresponding to the category\n        category = wiki_wiki.page(\"Category:\" + category_name)\n        \n        # Extract Wikipedia pages from the category and extend the list\n        wikipedia_pages.extend(extract_wikipedia_pages(wiki_wiki, category_name))\n        \n        # Add the explored category to the list\n        explored_categories.append(category_name)\n\n    # Extract subcategories and remove duplicate categories\n    categories_to_explore = [item.replace(\"Category:\", \"\") for item in wikipedia_pages if \"Category:\" in item]\n    wikipedia_pages = list(set([item for item in wikipedia_pages if \"Category:\" not in item]))\n    \n    # Explore subcategories recursively\n    while categories_to_explore:\n        category_name = categories_to_explore.pop()\n        print(f\"\\tExploring {category_name} on Wikipedia\")\n        \n        # Extract more references from the subcategory\n        more_refs = extract_wikipedia_pages(wiki_wiki, category_name)\n\n        # Iterate through the references\n        for ref in more_refs:\n            # Check if the reference is a category\n            if \"Category:\" in ref:\n                new_category = ref.replace(\"Category:\", \"\")\n                # Add the new category to the explored categories list\n                if new_category not in explored_categories:\n                    explored_categories.append(new_category)\n            else:\n                # Add the reference to the Wikipedia pages list\n                if ref not in wikipedia_pages:\n                    wikipedia_pages.append(ref)\n\n    # Initialize a list to store extracted texts\n    extracted_texts = []\n    \n    # Iterate through each Wikipedia page\n    print(\"- Processing Wikipedia pages:\")\n    for page_title in tqdm(wikipedia_pages):\n        try:\n            # Make a request to the Wikipedia page\n            page = wiki_wiki.page(page_title)\n\n            # Check if the page summary does not contain certain keywords\n            if \"Biden\" not in page.summary and \"Trump\" not in page.summary:\n                # Append the page title and summary to the extracted texts list\n                if len(page.summary) > len(page.title):\n                    extracted_texts.append(page.title + \" : \" + clean_string(page.summary))\n\n                # Iterate through the sections in the page\n                for section in page.sections:\n                    # Append the page title and section text to the extracted texts list\n                    if len(section.text) > len(page.title):\n                        extracted_texts.append(page.title + \" : \" + clean_string(section.text))\n                        \n        except Exception as e:\n            print(f\"Error processing page {page.title}: {e}\")\n                    \n    # Return the extracted texts\n    return extracted_texts","metadata":{"execution":{"iopub.status.busy":"2025-01-29T08:26:29.864958Z","iopub.execute_input":"2025-01-29T08:26:29.865249Z","iopub.status.idle":"2025-01-29T08:26:29.872925Z","shell.execute_reply.started":"2025-01-29T08:26:29.865228Z","shell.execute_reply":"2025-01-29T08:26:29.872092Z"},"trusted":true},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"To develop an AI assistant capable of answering questions about data science, I've chosen to begin with topics such as machine learning, data science, statistics, and deep learning artificial intelligence. As evident from the output, the range of topics it covers is truly impressive, even for a seasoned data scientist!\n","metadata":{}},{"cell_type":"code","source":"categories = [\"Machine_learning\", \"Data_science\", \"Statistics\", \"Deep_learning\", \"Artificial_intelligence\"]\nextracted_texts = get_wikipedia_pages(categories)\nprint(\"Found\", len(extracted_texts), \"Wikipedia pages\")","metadata":{"execution":{"iopub.status.busy":"2025-01-29T08:27:06.265208Z","iopub.execute_input":"2025-01-29T08:27:06.265591Z","iopub.status.idle":"2025-01-29T08:36:05.753999Z","shell.execute_reply.started":"2025-01-29T08:27:06.265549Z","shell.execute_reply":"2025-01-29T08:36:05.753116Z"},"trusted":true},"outputs":[{"name":"stdout","text":"- Processing Wikipedia categories:\n\tExploring Machine_learning on Wikipedia\n\tExploring Data_science on Wikipedia\n\tExploring Statistics on Wikipedia\n\tExploring Deep_learning on Wikipedia\n\tExploring Artificial_intelligence on Wikipedia\n\tExploring Artificial intelligence stubs on Wikipedia\n\tExploring Works created using artificial intelligence on Wikipedia\n\tExploring Turing tests on Wikipedia\n\tExploring AI safety on Wikipedia\n\tExploring Rule engines on Wikipedia\n\tExploring Regulation of artificial intelligence on Wikipedia\n\tExploring Problems in artificial intelligence on Wikipedia\n\tExploring Philosophy of artificial intelligence on Wikipedia\n\tExploring Open-source artificial intelligence on Wikipedia\n\tExploring Artificial intelligence laboratories on Wikipedia\n\tExploring Knowledge representation on Wikipedia\n\tExploring History of artificial intelligence on Wikipedia\n\tExploring Generative artificial intelligence on Wikipedia\n\tExploring Fuzzy logic on Wikipedia\n\tExploring Fiction about artificial intelligence on Wikipedia\n\tExploring Existential risk from artificial general intelligence on Wikipedia\n\tExploring Evolutionary computation on Wikipedia\n\tExploring Distributed artificial intelligence on Wikipedia\n\tExploring Deaths caused by robots and artificial intelligence on Wikipedia\n\tExploring Artificial intelligence conferences on Wikipedia\n\tExploring Artificial intelligence companies on Wikipedia\n\tExploring Automated reasoning on Wikipedia\n\tExploring Artificial intelligence associations on Wikipedia\n\tExploring Artificial intelligence templates on Wikipedia\n\tExploring Artificial intelligence publications on Wikipedia\n\tExploring Artificial intelligence people on Wikipedia\n\tExploring Artificial intelligence entertainment on Wikipedia\n\tExploring Artificial intelligence engineering on Wikipedia\n\tExploring Artificial intelligence competitions on Wikipedia\n\tExploring Artificial intelligence art on Wikipedia\n\tExploring Artificial immune systems on Wikipedia\n\tExploring Argument technology on Wikipedia\n\tExploring Applications of artificial intelligence on Wikipedia\n\tExploring Ambient intelligence on Wikipedia\n\tExploring AI supercomputers on Wikipedia\n\tExploring AI software on Wikipedia\n\tExploring AI accelerators on Wikipedia\n\tExploring Affective computing on Wikipedia\n\tExploring Text-to-video generation on Wikipedia\n\tExploring Text-to-image generation on Wikipedia\n\tExploring Google DeepMind on Wikipedia\n\tExploring Deepfakes on Wikipedia\n\tExploring Deep learning software on Wikipedia\n\tExploring Statistics stubs on Wikipedia\n\tExploring Statistical concepts on Wikipedia\n\tExploring Statistical software on Wikipedia\n\tExploring Statistical methods on Wikipedia\n\tExploring Statistical data on Wikipedia\n\tExploring Subfields of statistics on Wikipedia\n\tExploring Statistics profession and organizations on Wikipedia\n\tExploring Statistics-related lists on Wikipedia\n\tExploring Statisticians on Wikipedia\n\tExploring Data scientists on Wikipedia\n\tExploring Unsupervised learning on Wikipedia\n\tExploring Support vector machines on Wikipedia\n\tExploring Supervised learning on Wikipedia\n\tExploring Structured prediction on Wikipedia\n\tExploring Statistical natural language processing on Wikipedia\n\tExploring Semisupervised learning on Wikipedia\n\tExploring Natural language processing researchers on Wikipedia\n\tExploring Machine learning researchers on Wikipedia\n\tExploring Reinforcement learning on Wikipedia\n\tExploring Ontology learning (computer science) on Wikipedia\n\tExploring Markov models on Wikipedia\n\tExploring Machine learning task on Wikipedia\n\tExploring Machine learning algorithms on Wikipedia\n\tExploring Loss functions on Wikipedia\n\tExploring Log-linear models on Wikipedia\n\tExploring Learning in computer vision on Wikipedia\n\tExploring Kernel methods for machine learning on Wikipedia\n\tExploring Inductive logic programming on Wikipedia\n\tExploring Genetic programming on Wikipedia\n\tExploring Evolutionary algorithms on Wikipedia\n\tExploring Ensemble learning on Wikipedia\n\tExploring Dimension reduction on Wikipedia\n\tExploring Datasets in machine learning on Wikipedia\n\tExploring Data mining and machine learning software on Wikipedia\n\tExploring Signal processing conferences on Wikipedia\n\tExploring Artificial intelligence conferences on Wikipedia\n\tExploring Computational learning theory on Wikipedia\n\tExploring Cluster analysis on Wikipedia\n\tExploring Classification algorithms on Wikipedia\n\tExploring Blockmodeling on Wikipedia\n\tExploring Bayesian networks on Wikipedia\n\tExploring Artificial neural networks on Wikipedia\n\tExploring Applied machine learning on Wikipedia\n- Processing Wikipedia pages:\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 3250/3250 [08:33<00:00,  6.33it/s]","output_type":"stream"},{"name":"stdout","text":"Found 15061 Wikipedia pages\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"As a last step, the extracted knowledge base is saved to disk for later usage","metadata":{}},{"cell_type":"code","source":"wikipedia_data_science_kb = pd.DataFrame(extracted_texts, columns=[\"wikipedia_text\"])\nwikipedia_data_science_kb.to_csv(\"wikipedia_data_science_kb.csv\", index=False)\nwikipedia_data_science_kb.head()","metadata":{"execution":{"iopub.status.busy":"2025-01-29T09:13:07.731832Z","iopub.execute_input":"2025-01-29T09:13:07.732129Z","iopub.status.idle":"2025-01-29T09:13:08.037727Z","shell.execute_reply.started":"2025-01-29T09:13:07.732105Z","shell.execute_reply":"2025-01-29T09:13:08.036782Z"},"trusted":true},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"                                      wikipedia_text\n0  Data science : Data science is an interdiscipl...\n1  Data science : Data science is an interdiscipl...\n2  Data science : Data analysis typically involve...\n3  Data science : Cloud computing can offer acces...\n4  Data science : Data science involve collecting...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>wikipedia_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Data science : Data science is an interdiscipl...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Data science : Data science is an interdiscipl...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Data science : Data analysis typically involve...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Data science : Cloud computing can offer acces...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Data science : Data science involve collecting...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"wikipedia_data_science_kb.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T09:13:10.463739Z","iopub.execute_input":"2025-01-29T09:13:10.464035Z","iopub.status.idle":"2025-01-29T09:13:10.468976Z","shell.execute_reply.started":"2025-01-29T09:13:10.464013Z","shell.execute_reply":"2025-01-29T09:13:10.468188Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"(15061, 1)"},"metadata":{}}],"execution_count":19},{"cell_type":"markdown","source":"# 6. A test run","metadata":{}},{"cell_type":"markdown","source":"We instantiate it using the Gemma 2b-it and the gte-large embeddings and provide the extracts from Wikipedia as a knowledge base.\n\nThe Generate Text Embedding (gte) model is a variant of the BERT model developed by Alibaba DAMO Academy. This embedding model is available in three versions (large, base, small) and is specifically designed for English text processing. In comparisons with other embedding models, the gte-large variant demonstrates superior performance in retrieval tasks, but it also needs more storage space for embedding vectors compared to competitors (we do not worry much about that because ScaNN is quite fast for this application).\n\nThe instantiation will take a short while, then you can ask a few questions to the AI assistant.\n\n","metadata":{}},{"cell_type":"code","source":"# Initialize the name of the embeddings and model\nembeddings_name = \"thenlper/gte-large\"\nmodel_name = \"/kaggle/input/gemma/transformers/2b-it/1\"\n\n# Create an instance of AIAssistant with specified parameters\ngemma_ai_assistant = AIAssistant(gemma_model=GemmaHF(model_name), embeddings_name=embeddings_name)\n\n# Map the intended knowledge base to embeddings and index it\ngemma_ai_assistant.learn_knowledge_base(knowledge_base=extracted_texts)\n\n# Save the embeddings to disk (for later use)\ngemma_ai_assistant.save_embeddings()\n\n# Set the temperature (creativity) of the AI assistant and set the role\ngemma_ai_assistant.set_temperature(0.0)\ngemma_ai_assistant.set_role(\"data science expert whose explanations are useful, clear and complete\")","metadata":{"execution":{"iopub.status.busy":"2025-01-29T09:13:14.764601Z","iopub.execute_input":"2025-01-29T09:13:14.764944Z","iopub.status.idle":"2025-01-29T09:22:51.299068Z","shell.execute_reply.started":"2025-01-29T09:13:14.764917Z","shell.execute_reply":"2025-01-29T09:22:51.298232Z"},"trusted":true},"outputs":[{"name":"stdout","text":"\nInitializing model:\nPyTorch version: 2.5.1+cu121 -- using cuda\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"863ce75378704df8aac64d03641e5359"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/385 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e72e9fe1f76f4eeab126fa5dbc418936"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/67.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e3ea4c7086b40fc8aa681ad761421f0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/57.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94676ad730dd40a686d86fe39dafffaa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/619 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c86a901ca95d462b98db76f94564220a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/670M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"248410efc2a1468db56dd6615785a705"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/342 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6bd4d688706a46438463f3db3bb4f68d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a6c8f7ae80c44e9bef9b192645aef9e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/712k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d650f839b81f4caf8968394ca0f5eadb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1d0f545fb3c45e68258be0284ea62f1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"1_Pooling/config.json:   0%|          | 0.00/191 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d8d6350febf245498b4ff36125af3056"}},"metadata":{}},{"name":"stdout","text":"Indexing and mapping the knowledge base:\nMapping 15061 pieces of information\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 15061/15061 [08:39<00:00, 29.00it/s]\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"gemma_ai_assistant.query(\"What is the difference between data science, machine learning, and artificial intelligence?\")","metadata":{"execution":{"iopub.status.busy":"2025-01-27T15:17:31.401980Z","iopub.execute_input":"2025-01-27T15:17:31.402287Z","iopub.status.idle":"2025-01-27T15:17:58.383012Z","shell.execute_reply.started":"2025-01-27T15:17:31.402262Z","shell.execute_reply":"2025-01-27T15:17:58.382137Z"},"trusted":true},"outputs":[{"name":"stdout","text":"The difference between data science, machine learning, and artificial intelligence is as follows:\n\nData science is the study of data and the development of methods and tools for extracting knowledge and insights from data. It encompasses a wide range of techniques and tools, including data exploration, data wrangling, data cleaning, data transformation, and data analysis.\n\nMachine learning is a subfield of AI that focuses on developing algorithms and techniques to learn from data. Machine learning algorithms can be supervised, where the training data is labeled with the desired outcome, or unsupervised, where the algorithm discovers patterns in the data on its own.\n\nArtificial intelligence is the ability of machines to perform tasks that typically require human intelligence. This includes tasks such as natural language processing, image recognition, and decision-making.\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"gemma_ai_assistant.query(\"Explain how linear regression works\")","metadata":{"execution":{"iopub.status.busy":"2025-01-27T15:18:08.308823Z","iopub.execute_input":"2025-01-27T15:18:08.309117Z","iopub.status.idle":"2025-01-27T15:18:28.233384Z","shell.execute_reply.started":"2025-01-27T15:18:08.309096Z","shell.execute_reply":"2025-01-27T15:18:28.232409Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Sure, here's an explanation of how linear regression works:\n\nLinear regression is a statistical method used to predict a dependent variable based on one or more independent variables. It is a simple but powerful technique that can be used to model a wide range of relationships between the dependent and independent variables.\n\nThe basic idea behind linear regression is that we find a line that best fits the data points. The line should minimize the difference between the predicted values and the actual values.\n\nTo find the line that best fits the data, we use a least-squares approach. This approach minimizes the sum of the squared errors between the predicted values and the actual values.\n\nThe equation of a line that best fits the data can be written in the form:\n\ny = mx + b\n\nwhere:\n\ny is the dependent variable\nx is the independent variable\nm is the slope of the line\nb is the y-intercept\n\nThe slope of the line tells us how quickly the dependent variable changes with changes in the independent variable. The y-intercept tells us where the line crosses the y-axis.\n\nLinear regression is a powerful tool for modeling relationships between the dependent and independent variables. It is often used in various fields, such as finance, marketing, and science.\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"gemma_ai_assistant.query(\"What are decision trees, and how do they work in machine learning?\")","metadata":{"execution":{"iopub.status.busy":"2025-01-27T15:18:52.414618Z","iopub.execute_input":"2025-01-27T15:18:52.414928Z","iopub.status.idle":"2025-01-27T15:19:17.431836Z","shell.execute_reply.started":"2025-01-27T15:18:52.414906Z","shell.execute_reply":"2025-01-27T15:19:17.430925Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Sure, here's an answer to your question:\n\nWhat are Decision Trees?\n\nDecision trees are a supervised learning approach used in statistics, data mining, and machine learning. They are tree-based models used to classify or predict the value of a target variable based on several input variables.\n\nHow do Decision Trees Work?\n\nDecision trees work by iteratively splitting the data based on the most important features. The splitting criteria can be based on the target variable or on the features themselves. The tree continues to split until each node contains a small number of instances. The final tree is then used to make predictions on new data.\n\nTypes of Decision Trees\n\n* Classification Tree: Used for classification tasks.\n* Regression Tree: Used for regression tasks.\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}